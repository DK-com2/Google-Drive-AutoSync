#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ•ã‚©ãƒ«ãƒ€å†…ã®æ—¢å­˜éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™
"""

import sys
import json
import logging
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from drive_monitor import DriveMonitor
from file_processor import FileProcessor

def setup_logging():
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    log_dir = project_root / "logs"
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / f"existing_files_{datetime.now().strftime('%Y%m%d')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('ExistingFileProcessor')

def process_existing_files():
    """æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
    logger = setup_logging()
    logger.info("=== æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹ ===")
    
    try:
        # è¨­å®šèª­ã¿è¾¼ã¿
        config_file = project_root / "config" / "config.json"
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ãƒ¢ãƒ‹ã‚¿ãƒ¼ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–
        monitor = DriveMonitor(config)
        processor = FileProcessor(config)
        folder_id = config['google_drive']['target_folder_id']
        
        logger.info(f"å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ID: {folder_id}")
        
        # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—
        processed_files = monitor._get_processed_files()
        logger.info(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(processed_files)}")
        
        # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å–å¾—
        logger.info("ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å–å¾—ä¸­...")
        
        response = monitor.service.files().list(
            q=f"'{folder_id}' in parents and trashed=false",
            fields="files(id,name,size,md5Checksum,mimeType,modifiedTime,parents)",
            orderBy="modifiedTime desc"
        ).execute()
        
        all_files = response.get('files', [])
        logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€å†…ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_files)}")
        
        # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
        target_files = []
        for file_info in all_files:
            file_id = file_info['id']
            file_name = file_info['name']
            
            # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
            if file_id in processed_files:
                logger.debug(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {file_name}")
                continue
            
            # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
            if monitor._is_target_file(file_info):
                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒã‚§ãƒƒã‚¯
                if monitor._is_upload_complete(file_info):
                    target_files.append(file_info)
                    logger.info(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {file_name}")
                else:
                    logger.warning(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æœªå®Œäº†: {file_name}")
            else:
                logger.debug(f"å¯¾è±¡å¤–ãƒ•ã‚¡ã‚¤ãƒ«: {file_name}")
        
        if not target_files:
            logger.info("å‡¦ç†å¯¾è±¡ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Ÿè¡Œ
        logger.info(f"{len(target_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†é–‹å§‹")
        processed_count = 0
        
        for file_info in target_files:
            file_name = file_info['name']
            file_size = file_info.get('size', 0)
            
            try:
                logger.info(f"å‡¦ç†é–‹å§‹: {file_name} ({int(file_size)/1024/1024:.1f}MB)")
                
                success = processor.process_file(file_info)
                if success:
                    processed_count += 1
                    logger.info(f"âœ… å‡¦ç†å®Œäº†: {file_name}")
                else:
                    logger.error(f"âŒ å‡¦ç†å¤±æ•—: {file_name}")
                    
            except Exception as e:
                logger.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_name} - {str(e)}")
                continue
        
        # å‡¦ç†çµæœã¾ã¨ã‚
        logger.info(f"=== å‡¦ç†çµæœ: {processed_count}/{len(target_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº† ===")
        
        if processed_count > 0:
            results_dir = project_root / "data" / "results"
            logger.info(f"è§£æçµæœã¯ {results_dir} ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
    except Exception as e:
        logger.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    logger.info("=== æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµ‚äº† ===")
    return True

if __name__ == "__main__":
    success = process_existing_files()
    if success:
        print("\nğŸ‰ å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ“ è§£æçµæœ: data/results/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    else:
        print("\nâŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        print("ğŸ“„ ãƒ­ã‚°: logs/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
